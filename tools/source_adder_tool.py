# tools/source_adder_tool.py - VERSION CORRIG√âE POUR DIVERSIFIER LES SOURCES
import os
import time
import csv
import json
import pandas as pd
from typing import List, Dict, Optional, Tuple
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from transformers import AutoTokenizer
from smolagents import tool
from dotenv import load_dotenv
import re
from collections import defaultdict
import hashlib

load_dotenv(".env")

# Token HuggingFace
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
if HF_TOKEN is None:
    raise ValueError("La variable d'environnement HUGGINGFACE_TOKEN n'est pas d√©finie.")

# Tokenizer Mistral
tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    use_auth_token=os.getenv("HF_TOKEN")
)

class PreciseSourceMatcher:
    def __init__(self, driver: webdriver.Chrome, tokenizer, max_tokens: int = 512, csv_path: str = "azure_learning_chunks.csv"):
        self.driver = driver
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.csv_path = csv_path
        self.chunks_data = self._load_chunks_data()
        self.used_urls = set()  # CORRIG√â: Tracker des URLs utilis√©es (pas des hashes)
        self.used_modules = set()  # NOUVEAU: Tracker des modules utilis√©s
        self.used_units = set()
        print(f"üîç PreciseSourceMatcher initialis√© avec {len(self.chunks_data)} chunks")

    def _load_chunks_data(self) -> List[Dict]:
        """Charge tous les chunks en m√©moire avec leurs URLs sp√©cifiques"""
        chunks = []
        try:
            possible_paths = [
                self.csv_path,
                os.path.join(os.path.dirname(__file__), "azure_learning_chunks.csv"),
                os.path.join(os.getcwd(), "azure_learning_chunks.csv")
            ]
            
            actual_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    actual_path = path
                    break
            
            if not actual_path:
                print(f"‚ùå CSV file not found in paths: {possible_paths}")
                return []
            
            with open(actual_path, newline='', encoding="utf-8") as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    chunk = {
                        'module_name': row['module_name'],
                        'unit_name': row['unit_name'],
                        'unit_url': row['unit_url'],  # CRITIQUE: Cette URL doit √™tre diff√©rente pour chaque chunk
                        'chunk_id': row['chunk_id'],
                        'text_chunk': row['text_chunk'],
                        'token_count': int(row.get('token_count', 0))
                    }
                    
                    # Ajouter un hash unique pour r√©f√©rencement pr√©cis
                    chunk['content_hash'] = hashlib.md5(chunk['text_chunk'].encode()).hexdigest()[:8]
                    
                    # Cr√©er un titre de section bas√© sur le contenu
                    chunk['section_title'] = self._generate_section_title(chunk['text_chunk'])
                    
                    chunks.append(chunk)
            
            print(f"üìö Charg√© {len(chunks)} chunks depuis {actual_path}")
            
            # DIAGNOSTIC: V√©rifier la diversit√© des URLs
            unique_urls = set(chunk['unit_url'] for chunk in chunks)
            unique_modules = set(chunk['module_name'] for chunk in chunks)
            print(f"üîç DIAGNOSTIC: {len(unique_urls)} URLs uniques sur {len(chunks)} chunks")
            print(f"üîç DIAGNOSTIC: {len(unique_modules)} modules uniques")
            
            if len(unique_urls) < 5:
                print("‚ö†Ô∏è  PROBL√àME D√âTECT√â: Tr√®s peu d'URLs distinctes dans les donn√©es!")
                print("   URLs trouv√©es:")
                for url in list(unique_urls)[:10]:
                    print(f"     - {url}")
            
            return chunks
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des chunks : {e}")
            return []

    def _generate_section_title(self, text: str, max_length: int = 60) -> str:
        """G√©n√®re un titre de section repr√©sentatif du contenu du chunk"""
        lines = text.split('\n')
        
        # Chercher une ligne qui ressemble √† un titre
        for line in lines[:5]:
            line = line.strip()
            if line and (
                line.isupper() or 
                line.startswith('#') or 
                line.startswith('##') or
                line.startswith('**') or
                (len(line) < 80 and len(line) > 10)
            ):
                clean_title = re.sub(r'^#+\s*', '', line)
                clean_title = re.sub(r'^\*\*|\*\*$', '', clean_title)
                return clean_title[:max_length].strip()
        
        # Chercher des patterns de titres dans le texte
        title_patterns = [
            r'^\s*([A-Z][^.!?]*(?:Azure|Microsoft|Learning|AI|Machine|Data)[^.!?]*)',
            r'^\s*(\d+\.?\s*[A-Z][^.!?]{10,60})',
            r'^\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s*:)',
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, text, re.MULTILINE)
            if match:
                return match.group(1)[:max_length].strip()
        
        # Sinon, prendre la premi√®re phrase substantielle
        sentences = re.split(r'[.!?]+', text)
        for sentence in sentences:
            sentence = sentence.strip()
            if 15 <= len(sentence) <= 100:
                return sentence[:max_length]
        
        # Fallback : premi√®res mots du texte
        words = text.split()[:10]
        return ' '.join(words)[:max_length]

    def _calculate_relevance_score(self, chunk_text: str, keywords: List[str], chunk_data: Dict = None, exclude_used_sources: bool = True) -> Tuple[float, Dict]:
        """Calcule un score de pertinence avec p√©nalit√© pour sources d√©j√† utilis√©es"""
        text_lower = chunk_text.lower()
        score_details = {
            'exact_matches': 0,
            'partial_matches': 0,
            'keyword_density': 0,
            'position_bonus': 0,
            'semantic_bonus': 0,
            'diversity_bonus': 0,
            'matched_keywords': [],
            'diversity_penalty': False
        }
        score = 0.0
        
        # CORRIG√â: P√©nalit√© bas√©e sur l'URL ET le module
        if exclude_used_sources and chunk_data:
            chunk_url = chunk_data.get('unit_url', '')
            chunk_module = chunk_data.get('unit_name', '')  # ‚úÖ unit√© p√©dagogique plus fine
            
            # P√©nalit√© forte si URL d√©j√† utilis√©e
            if chunk_url in self.used_urls:
                score -= 15.0  # P√©nalit√© tr√®s forte pour URL dupliqu√©e
                score_details['diversity_penalty'] = True
                print(f"   ‚ùå URL d√©j√† utilis√©e: {chunk_url[:50]}...")
            
            # P√©nalit√© mod√©r√©e si m√™me module (mais URL diff√©rente)
            elif chunk_module in self.used_units:
                score -= 5.0  # P√©nalit√© mod√©r√©e pour module d√©j√† utilis√©
                score_details['module_reuse'] = True
                print(f"   ‚ö†Ô∏è  Module d√©j√† utilis√©: {chunk_module}")
            
            # Bonus pour nouvelle URL et nouveau module
            else:
                score_details['diversity_bonus'] = 3.0
                score += 3.0
        
        # 1. Compter les occurrences exactes avec position
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if len(keyword_lower) < 3:
                continue
                
            exact_count = text_lower.count(keyword_lower)
            if exact_count > 0:
                score_details['exact_matches'] += exact_count
                score_details['matched_keywords'].append(keyword)
                
                # Score variable selon l'importance du mot-cl√©
                if len(keyword_lower) > 8:  # Mots techniques longs
                    score += exact_count * 5.0
                elif keyword_lower in ['azure', 'microsoft', 'learning', 'machine', 'ai', 'data']:
                    score += exact_count * 4.0
                else:
                    score += exact_count * 3.0
                
                # Bonus si le mot-cl√© appara√Æt t√¥t dans le texte
                first_occurrence = text_lower.find(keyword_lower)
                if first_occurrence < len(text_lower) * 0.2:
                    score_details['position_bonus'] += 1
                    score += 2.0
        
        # 2. Correspondances partielles et variantes
        words = re.findall(r'\b\w+\b', text_lower)
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if len(keyword_lower) < 4:
                continue
            
            partial_matches = sum(1 for word in words 
                                if keyword_lower in word and word != keyword_lower and len(word) > len(keyword_lower))
            if partial_matches > 0:
                score_details['partial_matches'] += partial_matches
                score += partial_matches * 1.5
        
        # 3. Bonus s√©mantique pour co-occurrence de termes li√©s
        semantic_groups = [
            ['machine', 'learning', 'apprentissage', 'mod√®le', 'algorithme'],
            ['azure', 'microsoft', 'cloud', 'service'],
            ['data', 'donn√©es', 'dataset', 'base'],
            ['neural', 'r√©seau', 'neurone', 'deep'],
            ['nlp', 'natural', 'language', 'processing', 'text', 'linguistique'],
            ['computer', 'vision', 'image', 'reconnaissance', 'visuelle'],
            ['speech', 'voice', 'audio', 'parole', 'reconnaissance']
        ]
        
        for group in semantic_groups:
            group_matches = sum(1 for term in group if term in text_lower)
            if group_matches >= 2:
                score_details['semantic_bonus'] += group_matches
                score += group_matches * 1.0
        
        # 4. Densit√© de mots-cl√©s
        total_words = len(words)
        if total_words > 0:
            matched_keywords_count = len(score_details['matched_keywords'])
            density = matched_keywords_count / len(keywords) if keywords else 0
            score_details['keyword_density'] = round(density, 3)
            score += density * 5.0
        
        # 5. Bonus pour les chunks courts mais denses
        if total_words < 200 and score > 5:
            score += 2.0
        
        # 6. P√©nalit√© pour chunks tr√®s longs avec peu de matches
        if total_words > 500 and score < 3:
            score *= 0.5
        
        return round(score, 2), score_details

    def _create_precise_reference(self, chunk_data: Dict) -> Dict:
        """Cr√©e une r√©f√©rence pr√©cise utilisant l'URL SP√âCIFIQUE du chunk"""
        specific_url = chunk_data['unit_url']
        chunk_id = chunk_data['chunk_id']
        content_hash = chunk_data['content_hash']
        section_title = chunk_data['section_title']
        
        # CORRIG√â: Nettoyer l'URL et cr√©er les variantes
        if '#' in specific_url:
            base_url = specific_url.split('#')[0]
            primary_url = specific_url  # Garder l'URL compl√®te avec ancre
        else:
            base_url = specific_url
            primary_url = f"{specific_url}#chunk-{chunk_id}"
        
        references = {
            'primary_url': primary_url,  # URL avec ancre sp√©cifique
            'base_url': base_url,        # URL de base propre
            'chunk_url': f"{base_url}#chunk-{chunk_id}",
            'hash_url': f"{base_url}#content-{content_hash}",
            'section_url': f"{base_url}#{self._url_safe_string(section_title)}",
            'chunk_identifier': f"{chunk_data['module_name']}/chunk-{chunk_id}",
            'section_title': section_title,
            'content_snippet': chunk_data['text_chunk'][:200] + "..." if len(chunk_data['text_chunk']) > 200 else chunk_data['text_chunk'],
            'is_specific_chunk': True
        }
        
        return references

    def _url_safe_string(self, text: str) -> str:
        """Convertit un texte en format URL-safe pour les ancres"""
        safe = re.sub(r'[^\w\-_]', '-', text.lower())
        safe = re.sub(r'-+', '-', safe)
        return safe.strip('-')[:50]

    def _extract_question_specific_keywords(self, question_text: str) -> List[str]:
        """
        Extrait des mots-cl√©s sp√©cifiques √† la question pos√©e
        """
        keywords = []
        text_lower = question_text.lower()
        
        # 1. Termes techniques sp√©cialis√©s dans la question
        technical_patterns = [
            # NLP et Language
            r'\b(?:nlp|natural language processing|linguistique|text|texte|sentiment|entity|entities|entit√©s|tokenization|lemmatization)\b',
            # Computer Vision
            r'\b(?:computer vision|vision|image|images|reconnaissance|detection|classification|opencv|cnn|convolution)\b',
            # Speech/Audio
            r'\b(?:speech|voice|audio|parole|reconnaissance vocale|text-to-speech|speech-to-text|phon√®me)\b',
            # Azure Services
            r'\b(?:cognitive services|bot framework|luis|qna maker|custom vision|face api|speech service|text analytics)\b',
            # ML G√©n√©ral
            r'\b(?:machine learning|deep learning|neural network|algorithm|model|training|supervised|unsupervised|clustering|regression|classification)\b',
            # Azure ML sp√©cifique
            r'\b(?:azure ml|ml studio|automl|pipeline|compute|workspace|datastore|experiment)\b'
        ]
        
        for pattern in technical_patterns:
            matches = re.findall(pattern, text_lower)
            keywords.extend(matches)
        
        # 2. Concepts sp√©cifiques mentionn√©s
        concept_keywords = {
            'analyse sentiment': ['sentiment', 'analysis', 'emotion', 'opinion'],
            'reconnaissance entit√©s': ['entity', 'entities', 'ner', 'named entity'],
            'classification': ['classification', 'classifier', 'category', 'classe'],
            'd√©tection objets': ['object detection', 'detection', 'bounding box', 'yolo'],
            'reconnaissance faciale': ['face recognition', 'facial', 'biometric', 'visage'],
            'traitement image': ['image processing', 'opencv', 'filter', 'transformation'],
            'bot': ['bot', 'chatbot', 'conversation', 'dialog', 'assistant'],
            'apis': ['api', 'rest', 'endpoint', 'service', 'integration']
        }
        
        for concept, related_terms in concept_keywords.items():
            if concept in text_lower:
                keywords.extend(related_terms)
        
        # 3. Extraire les mots significatifs uniques de la question
        question_words = re.findall(r'\b[a-zA-Z√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√¶≈ì√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ñ√ô√õ√ú≈∏√Ü≈í]{4,}\b', question_text.lower())
        
        # Filtrer les mots courants
        stop_words = {
            'question', 'r√©ponse', 'suivant', 'suivante', 'quelle', 'quelles', 'comment', 'pourquoi',
            'permet', 'utilise', 'faire', 'cr√©er', 'service', 'azure', 'microsoft'
        }
        
        significant_words = [word for word in question_words if word not in stop_words and len(word) > 3]
        keywords.extend(significant_words)
        
        # D√©dupliquer et retourner
        return list(set([kw.lower() for kw in keywords if kw and len(kw) > 2]))

    def retrieve_diverse_sources(self, question_text: str, max_results: int = 3, force_diversity: bool = True) -> List[Dict]:
        """
        Recherche avec diversification forc√©e des sources
        """
        if not self.chunks_data:
            print("‚ùå Aucune donn√©e de chunk disponible")
            return []
        
        print(f"üéØ Recherche DIVERSE pour: {question_text[:60]}...")
        
        # Extraire les mots-cl√©s sp√©cifiques √† cette question
        enriched_question = self._enrich_question(question_text)
        question_keywords = self._extract_question_specific_keywords(enriched_question)
        print(f"   üîç Mots-cl√©s: {question_keywords[:6]}")
        
        # Calculer les scores pour chaque chunk
        scored_chunks = []
        for chunk in self.chunks_data:
            score, score_details = self._calculate_relevance_score(
                chunk['text_chunk'], 
                question_keywords, 
                chunk_data=chunk,  # CORRIG√â: Passer les donn√©es du chunk
                exclude_used_sources=force_diversity
            )
            
            if score > 0.5:  # Seuil plus bas pour permettre plus de diversit√©
                scored_chunks.append((score, chunk, score_details))
        
        # Trier par score d√©croissant
        scored_chunks.sort(key=lambda x: x[0], reverse=True)
        
        print(f"üìä {len(scored_chunks)} chunks pertinents trouv√©s")
        
        # Algorithme de diversification AM√âLIOR√â
        selected_sources = []
        
        for score, chunk, score_details in scored_chunks:
            if len(selected_sources) >= max_results:
                break
            
            # V√©rifier la diversit√© stricte
            chunk_url = chunk['unit_url']
            chunk_module = chunk['module_name']
            
            # CORRIG√â: Diversit√© stricte par URL
            if force_diversity and chunk_url in self.used_urls:
                print(f"   ‚è≠Ô∏è  Ignor√© (URL d√©j√† utilis√©e): {chunk_url[:50]}...")
                continue
            
            # S√©lectionner cette source
            references = self._create_precise_reference(chunk)
            
            result = {
                "chunk_id": chunk['chunk_id'],
                "content_hash": chunk['content_hash'],
                "module_name": chunk['module_name'],
                "unit_name": chunk['unit_name'],
                "section_title": chunk['section_title'],
                "full_text": chunk['text_chunk'],
                "content_snippet": references['content_snippet'],
                "token_count": chunk['token_count'],
                "references": references,
                "relevance_score": score,
                "score_details": score_details,
                "matched_keywords": score_details['matched_keywords'],
                "search_metadata": {
                    "chunk_size": len(chunk['text_chunk']),
                    "keyword_coverage": len(score_details['matched_keywords']) / len(question_keywords) if question_keywords else 0,
                    "uses_specific_url": True,
                    "diversity_enforced": force_diversity,
                    "url_is_unique": chunk_url not in self.used_urls,
                    "module_is_unique": chunk_module not in self.used_modules
                }
            }
            
            selected_sources.append(result)
            
            # CORRIG√â: Marquer les sources comme utilis√©es
            if force_diversity:
                self.used_urls.add(chunk_url)
                self.used_modules.add(chunk_module)
                self.used_units.add(chunk['unit_name'])  # ‚úÖ Nouvelle r√®gle
            
            print(f"  ‚úÖ S√©lectionn√©: {chunk_module} - Score {score}")
            print(f"     üîó URL: {chunk_url}")
        
        print(f"üéâ {len(selected_sources)} sources diverses s√©lectionn√©es")
        return selected_sources

    def get_best_sources_for_quiz_question(self, quiz_question: str, options: List[str] = None, explanation: str = None, max_sources: int = 3) -> List[Dict]:
        """
        Recherche bas√©e sur la question SP√âCIFIQUE du quiz avec diversification
        """
        print(f"üéØ Recherche pour question: {quiz_question[:80]}...")
        
        # Utiliser la nouvelle m√©thode de recherche diverse
        return self.retrieve_diverse_sources(
            question_text=quiz_question,
            max_results=max_sources,
            force_diversity=True
        )

    def reset_used_sources(self):
        """Remet √† z√©ro les trackers de sources utilis√©es"""
        self.used_urls.clear()
        self.used_modules.clear()
        print("üîÑ Sources utilis√©es remises √† z√©ro")

    def _enrich_question(self, raw_question: str) -> str:
        """Reformule la question de l'utilisateur pour am√©liorer la recherche"""
        return f"Quels sont les concepts ou services cl√©s √©voqu√©s dans cette question : ¬´ {raw_question.strip()} ¬ª ?"

import json
import pandas as pd
import re
from smolagents import tool
from difflib import get_close_matches

# Chargement du CSV des unit√©s locales (adapt√© √† ton format actuel)
df_sources = pd.read_csv("azure_learning_chunks.csv")  # colonnes : module_name, unit_name, unit_url

@tool
def add_precise_sources_to_quiz_tool(quiz_string: str, num_relevant_sources: int = 3) -> str:
    """
    Enrichit chaque question du quiz avec des sources pertinentes (unit√©s d'apprentissage).

    Args:
        quiz_string (str): Cha√Æne JSON contenant les questions.
        num_relevant_sources (int): Nombre maximal de sources √† ajouter par question.

    Returns:
        str: Cha√Æne JSON enrichie avec les sources pertinentes.
    """
    questions = json.loads(quiz_string)

    for question in questions:
        texte_question = question["question"]
        mots_cles = extraire_mots_cles(texte_question)

        # Recherche dans le DataFrame des unit√©s
        scores = []
        for _, row in df_sources.iterrows():
            titre_complet = f"{row['module_name']} - {row['unit_name']}".lower()
            score = sum(1 for mot in mots_cles if mot in titre_complet)
            if score > 0:
                scores.append((score, row['module_name'], row['unit_name'], row['unit_url']))

        # Tri des meilleures correspondances
        scores = sorted(scores, reverse=True)[:num_relevant_sources]

        question["sources"] = [
            {
                "module": module,
                "title": unit,
                "url": url
            }
            for _, module, unit, url in scores
        ]

    return json.dumps(questions, indent=2, ensure_ascii=False)


def extraire_mots_cles(texte: str):
    # Enl√®ve les mots trop courts et les stopwords simples
    stopwords = {"les", "des", "une", "avec", "dans", "pour", "quoi", "que", "comment", "est", "ce", "qui"}
    mots = re.findall(r"\b\w+\b", texte.lower())
    return [mot for mot in mots if mot not in stopwords and len(mot) > 3]


def create_driver() -> webdriver.Chrome:
    """Cr√©e un driver Chrome headless optimis√©"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-logging")
    chrome_options.add_argument("--silent")
    return webdriver.Chrome(options=chrome_options)

# Test de diversification
# Test de diversification des sources - Version compl√®te
import sys
from collections import Counter
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

def setup_chrome_driver():
    """Configure et initialise le driver Chrome"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def analyze_source_diversity(chunks_data):
    """Analyse la diversit√© des sources dans les donn√©es"""
    if not chunks_data:
        print("‚ùå Aucune donn√©e √† analyser")
        return None
    
    urls = [chunk.get('unit_url', 'Unknown') for chunk in chunks_data]
    modules = [chunk.get('module_name', 'Unknown') for chunk in chunks_data]
    
    # Compter les occurrences
    url_counts = Counter(urls)
    module_counts = Counter(modules)
    
    # Calculer les m√©triques de diversit√©
    total_chunks = len(chunks_data)
    unique_urls = len(set(urls))
    unique_modules = len(set(modules))
    
    # Indice de diversit√© (Shannon)
    from math import log
    def shannon_diversity(counts):
        total = sum(counts.values())
        if total == 0:
            return 0
        return -sum((count/total) * log(count/total) for count in counts.values() if count > 0)
    
    url_diversity = shannon_diversity(url_counts)
    module_diversity = shannon_diversity(module_counts)
    
    return {
        'total_chunks': total_chunks,
        'unique_urls': unique_urls,
        'unique_modules': unique_modules,
        'url_counts': url_counts,
        'module_counts': module_counts,
        'url_diversity': url_diversity,
        'module_diversity': module_diversity,
        'diversity_ratio_urls': unique_urls / total_chunks if total_chunks > 0 else 0,
        'diversity_ratio_modules': unique_modules / total_chunks if total_chunks > 0 else 0
    }

def print_diversity_report(analysis):
    """Affiche un rapport d√©taill√© de la diversit√©"""
    if not analysis:
        return
    
    print(f"üìä RAPPORT DE DIVERSIT√â DES SOURCES")
    print("=" * 60)
    
    # M√©triques g√©n√©rales
    print(f"üìà M√âTRIQUES G√âN√âRALES:")
    print(f"   Total chunks analys√©s: {analysis['total_chunks']:,}")
    print(f"   URLs uniques: {analysis['unique_urls']:,}")
    print(f"   Modules uniques: {analysis['unique_modules']:,}")
    print(f"   Ratio diversit√© URLs: {analysis['diversity_ratio_urls']:.2%}")
    print(f"   Ratio diversit√© Modules: {analysis['diversity_ratio_modules']:.2%}")
    
    # Indices de diversit√© Shannon
    print(f"\nüî¨ INDICES DE DIVERSIT√â (Shannon):")
    print(f"   Diversit√© URLs: {analysis['url_diversity']:.3f}")
    print(f"   Diversit√© Modules: {analysis['module_diversity']:.3f}")
    
    # Top URLs
    print(f"\nüèÜ TOP 10 URLS LES PLUS UTILIS√âES:")
    for i, (url, count) in enumerate(analysis['url_counts'].most_common(10), 1):
        percentage = (count / analysis['total_chunks']) * 100
        print(f"   {i:2d}. {url[:60]}... ({count:,} chunks, {percentage:.1f}%)")
    
    # Top Modules
    print(f"\nüìö TOP 10 MODULES LES PLUS UTILIS√âS:")
    for i, (module, count) in enumerate(analysis['module_counts'].most_common(10), 1):
        percentage = (count / analysis['total_chunks']) * 100
        print(f"   {i:2d}. {module} ({count:,} chunks, {percentage:.1f}%)")
    
    # √âvaluation de la diversit√©
    print(f"\nüí° √âVALUATION DE LA DIVERSIT√â:")
    if analysis['diversity_ratio_urls'] > 0.7:
        print("   ‚úÖ Excellente diversit√© des URLs")
    elif analysis['diversity_ratio_urls'] > 0.5:
        print("   ‚ö†Ô∏è  Diversit√© des URLs mod√©r√©e")
    else:
        print("   ‚ùå Faible diversit√© des URLs - risque de sur-repr√©sentation")
    
    if analysis['diversity_ratio_modules'] > 0.3:
        print("   ‚úÖ Bonne diversit√© des modules")
    else:
        print("   ‚ö†Ô∏è  Diversit√© des modules limit√©e")

def test_source_matching_quality(matcher, sample_queries):
    """Test la qualit√© du matching des sources"""
    print(f"\nüß™ TEST DE QUALIT√â DU MATCHING")
    print("-" * 40)
    
    for i, query in enumerate(sample_queries, 1):
        print(f"\nüìù Test {i}: '{query}'")
        try:
            # Simuler une recherche
            results = matcher.find_relevant_sources(query, top_k=5)
            if results:
                print(f"   ‚úÖ {len(results)} sources trouv√©es")
                # Analyser la diversit√© des r√©sultats
                result_urls = [r.get('url', 'Unknown') for r in results]
                unique_result_urls = len(set(result_urls))
                print(f"   üéØ Diversit√©: {unique_result_urls}/{len(results)} URLs uniques")
            else:
                print("   ‚ùå Aucune source trouv√©e")
        except Exception as e:
            print(f"   ‚ùå Erreur: {str(e)}")

if __name__ == "__main__":
    print("üéØ TEST DE DIVERSIFICATION DES SOURCES - VERSION CORRIG√âE")
    print("=" * 80)
    
    # Configuration
    driver = None
    matcher = None
    
    try:
        # Initialiser le driver
        print("üîß Initialisation du driver Chrome...")
        driver = setup_chrome_driver()
        
        # Cr√©er le matcher (assuming PreciseSourceMatcher exists)
        print("üîß Initialisation du matcher...")
        # matcher = PreciseSourceMatcher(driver=driver, tokenizer=tokenizer)
        
        # Pour la d√©monstration, cr√©er des donn√©es factices
        print("üìä G√©n√©ration de donn√©es de test...")
        sample_chunks_data = [
            {'unit_url': 'https://example1.com/page1', 'module_name': 'module_a', 'content': 'Sample content 1'},
            {'unit_url': 'https://example1.com/page2', 'module_name': 'module_a', 'content': 'Sample content 2'},
            {'unit_url': 'https://example2.com/page1', 'module_name': 'module_b', 'content': 'Sample content 3'},
            {'unit_url': 'https://example3.com/page1', 'module_name': 'module_c', 'content': 'Sample content 4'},
            {'unit_url': 'https://example1.com/page3', 'module_name': 'module_a', 'content': 'Sample content 5'},
        ] * 100  # Multiplier pour avoir plus de donn√©es
        
        # DIAGNOSTIC: Analyser les donn√©es
        print(f"\nüîç ANALYSE DES DONN√âES:")
        analysis = analyze_source_diversity(sample_chunks_data)
        
        if analysis:
            print_diversity_report(analysis)
            
            # Test de qualit√© du matching
            sample_queries = [
                "recherche exemple 1",
                "module sp√©cifique",
                "contenu technique",
                "information g√©n√©rale"
            ]
            
            # D√©commenter si PreciseSourceMatcher est disponible
            # test_source_matching_quality(matcher, sample_queries)
            
        print(f"\n‚úÖ Test termin√© avec succ√®s!")
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Nettoyage
        if driver:
            print("üßπ Fermeture du driver...")
            driver.quit()
        
        print("üèÅ Fin du test de diversification")
